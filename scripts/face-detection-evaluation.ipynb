{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection on masked and non-masked images\n",
    "\n",
    "In this notebook we apply the RetinaFace face detector (https://github.com/peteryuX/retinaface-tf2) to our collected images.  \n",
    "The resulting cropped faces will be used to evaluate the complete pipeline (i.e. detector and mask/no_mask classifier) in evaluate_pipeline.ipynb.  \n",
    "We also plot some results of the detector to get an idea of its performance on both unmasked and masked faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "root_dir = os.path.dirname(os.path.abspath(os.curdir))\n",
    "# Add retinaface-tf2 submodule to PATH\n",
    "retinaface_dir = os.path.join(root_dir, 'retinaface-tf2')\n",
    "if retinaface_dir not in sys.path: # add retinaface-tf2 repo to PATH\n",
    "    sys.path.append(retinaface_dir)\n",
    "retinaface_cfg_path = Path(retinaface_dir) / 'configs' / 'retinaface_mbv2.yaml'\n",
    "\n",
    "from modules.models import RetinaFaceModel\n",
    "from modules.utils import pad_input_image, recover_pad_output, load_yaml\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.dirname(os.path.abspath(os.curdir))\n",
    "data_dir = Path(root_dir) / 'data'\n",
    "raw_images_dir = data_dir / 'raw_images'\n",
    "\n",
    "# this file contain ground truth annotations, detector bboxes will be compared to this\n",
    "ground_truth_annotations_fn = data_dir / 'test_validation_metadata.csv'\n",
    "\n",
    "# this is an output file, detector bboxes will be written here\n",
    "# these are needed to evaluate the complete pipeline, see evaluate_pipeline.ipynb\n",
    "detector_bbox_annotations_fn = data_dir / 'detector_annotations.csv'\n",
    "\n",
    "# these are the cropped faces corresponding to bboxes produced by the detector\n",
    "# these will serve as input to the mask/no_mask classifier in the evaluation of the pipeline \n",
    "detector_crop_output = data_dir / 'overall_model_evaluation' / 'detector_crops'\n",
    "detector_crop_output.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# this will be used to store the results of the detector, and to plot some of these at the end of this notebook\n",
    "detector_result_dir = data_dir / 'overall_model_evaluation' / 'detector_results'\n",
    "detector_result_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the RetinaFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toon/mask_env_37/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff703ef52d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retinaface_cfg = load_yaml(retinaface_cfg_path)\n",
    "\n",
    "# define network\n",
    "model = RetinaFaceModel(retinaface_cfg, training=False, iou_th=0.4, score_th=0.5)\n",
    "\n",
    "# load checkpoint\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "checkpoint_dir = os.path.join('../retinaface-tf2', retinaface_cfg['sub_name'])\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect faces and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_fresh/face-mask-detection/data/raw_images/101.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_fresh/face-mask-detection/data/raw_images/130.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_fresh/face-mask-detection/data/raw_images/131.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_fresh/face-mask-detection/data/raw_images/133.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_fresh/face-mask-detection/data/raw_images/149.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_fresh/face-mask-detection/data/raw_images/192.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_fresh/face-mask-detection/data/raw_images/245.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_fresh/face-mask-detection/data/raw_images/46.jpg\n"
     ]
    }
   ],
   "source": [
    "detector_bbox = pd.DataFrame(columns=['img','bbox_id','x','y','w','h','crop_fn'])\n",
    "real_raw_img = list(sorted(raw_images_dir.glob('*.jpg')))\n",
    "\n",
    "for idx, fn in enumerate(real_raw_img):\n",
    "    \n",
    "    img_raw = cv2.imread(str(fn))\n",
    "    img_height, img_width, _ = img_raw.shape\n",
    "    img = np.float32(img_raw.copy())\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    img, pad_params = pad_input_image(img, max_steps=max(retinaface_cfg['steps']))\n",
    "    \n",
    "    outputs = model(img[np.newaxis, ...]).numpy()\n",
    "    outputs = recover_pad_output(outputs, pad_params)\n",
    "\n",
    "    for prior_index in range(len(outputs)):\n",
    "        ann = outputs[prior_index]\n",
    "        x1, y1, x2, y2 = int(ann[0] * img_width), int(ann[1] * img_height), \\\n",
    "                         int(ann[2] * img_width), int(ann[3] * img_height)\n",
    "        \n",
    "        cropped_face = img[y1:y2, x1:x2]\n",
    "        \n",
    "        if cropped_face.shape[0] == 0 or cropped_face.shape[1] == 0:\n",
    "            print(f\"Detector extracted a face with 0 rows or columns, skipping this one for {fn}\")\n",
    "            continue\n",
    "\n",
    "        output_fn = detector_crop_output / (fn.stem + '_' + str(prior_index) + '.jpg')\n",
    "        cv2.imwrite(str(output_fn), cv2.cvtColor(cropped_face, cv2.COLOR_RGB2BGR))     \n",
    "        \n",
    "        detector_bbox.loc[len(detector_bbox)] = [fn.name,prior_index,x1,y1,x2-x1,y2-y1, str(output_fn)]\n",
    "    \n",
    "detector_bbox.to_csv(detector_bbox_annotations_fn, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare detected faces to ground truth annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to comput IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    '''\n",
    "    from https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "    '''\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write images to file with ground truth and detector annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(ground_truth_annotations_fn)\n",
    "ground_truth['img_name'] = ground_truth.raw_img_dir.apply(lambda x: Path(x).name)\n",
    "\n",
    "fn_to_gt_boxes = dict()\n",
    "fn_to_detector_boxes = dict()\n",
    "\n",
    "for idx, img_fn in enumerate(real_raw_img):\n",
    "\n",
    "    img = cv2.imread(str(img_fn))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(img/255)\n",
    "    \n",
    "    ground_truth_fn = ground_truth[ground_truth.img_name == img_fn.name]\n",
    "    gt_boxes = []\n",
    "    for row_idx in range(ground_truth_fn.shape[0]):\n",
    "        ground_truth_annotation = ground_truth_fn.iloc[row_idx]\n",
    "        x = ground_truth_annotation.x.item()\n",
    "        y = ground_truth_annotation.y.item()\n",
    "        w = ground_truth_annotation.w.item()\n",
    "        h = ground_truth_annotation.h.item()\n",
    "        gt_boxes.append((x,y,x+w,y+h))\n",
    "        rect = plt.Rectangle((x,y),w,h,linewidth=3,edgecolor='green',facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "    fn_to_gt_boxes[img_fn] = gt_boxes\n",
    "    \n",
    "    bbox_fn = detector_bbox[detector_bbox.img == img_fn.name]\n",
    "    pred_boxes = []\n",
    "    for row_idx in range(bbox_fn.shape[0]):\n",
    "        pred_annotation = bbox_fn.iloc[row_idx]\n",
    "        x = pred_annotation.x\n",
    "        y = pred_annotation.y\n",
    "        w = pred_annotation.w\n",
    "        h = pred_annotation.h\n",
    "        pred_boxes.append((x,y,x+w,y+h))\n",
    "        rect = plt.Rectangle((x,y),w,h,linewidth=3,edgecolor='red',facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "    fn_to_detector_boxes[img_fn] = pred_boxes\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.savefig(detector_result_dir / img_fn.name, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute performance statistics for the detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "correct = []\n",
    "mistakes = []\n",
    "    \n",
    "for idx, img_fn in enumerate(real_raw_img):\n",
    "    pred_boxes = fn_to_detector_boxes[img_fn]\n",
    "    gt_boxes = fn_to_gt_boxes[img_fn]\n",
    "    \n",
    "    # we have to keep track of previous fn and fp to see if there was a mistake\n",
    "    # in this particular image\n",
    "    fn_prev = fn\n",
    "    fp_prev = fp\n",
    "    \n",
    "    # first we check for each ground truth bounding box whether it was detected\n",
    "    for gt_box in gt_boxes:\n",
    "        times_found = 0\n",
    "        for pred_box in pred_boxes: \n",
    "            iou = bb_intersection_over_union(gt_box, pred_box)\n",
    "            if iou >= 0.5:\n",
    "                times_found += 1\n",
    "        \n",
    "        if times_found > 0:\n",
    "            # the face was detected at least once, so it is a true positive\n",
    "            tp += 1\n",
    "        \n",
    "            # if it is detected more than once, we add the duplicate detections to the false positives\n",
    "            n_duplicates = times_found - 1\n",
    "            if n_duplicates > 0:\n",
    "                fp += n_duplicates\n",
    "        \n",
    "        if times_found == 0:\n",
    "            fn += 1\n",
    "        \n",
    "    # we also have to check for detected bounding boxes that do not overlap with any ground truth bounding box,\n",
    "    # as these are counted as false negatives\n",
    "    for pred_box in pred_boxes:\n",
    "        found = False\n",
    "        for gt_box in gt_boxes:\n",
    "            iou = bb_intersection_over_union(gt_box, pred_box)\n",
    "            if iou > 0.5:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            fp += 1\n",
    "            \n",
    "    if fn_prev != fn or fp_prev != fp:\n",
    "        mistakes.append(detector_result_dir / img_fn.name)\n",
    "    else:\n",
    "        correct.append(detector_result_dir / img_fn.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of true positives: {tp}\")\n",
    "print(f\"Number of false positives: {fp}\")\n",
    "print(f\"Number of false negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some good and bad cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_plot = 16\n",
    "to_plot = random.sample(correct,n_to_plot)\n",
    "\n",
    "nrow = np.ceil(np.sqrt(n_to_plot))\n",
    "ncol = nrow\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, x in enumerate(to_plot):\n",
    "    plt.subplot(nrow, ncol, idx+1)\n",
    "    img = cv2.imread(str(x))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample of images with all faces detected correctly', size=20, y=0.9)\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_plot = 16\n",
    "to_plot = random.sample(mistakes,n_to_plot)\n",
    "\n",
    "nrow = np.ceil(np.sqrt(n_to_plot))\n",
    "ncol = nrow\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, x in enumerate(to_plot):\n",
    "    plt.subplot(nrow, ncol, idx+1)\n",
    "    img = cv2.imread(str(x))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample of images with mistakes in face detection', size=20, y=0.9)\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
