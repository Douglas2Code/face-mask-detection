{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Masked Face or Not Masked Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add submodule retinaface-tf2 to detect faces\n",
    "import os, sys\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sub_module_dir = os.path.join(ROOT_DIR, 'retinaface-tf2')\n",
    "if sub_module_dir not in sys.path: # add retinaface-tf2 repo to PATH\n",
    "    sys.path.append(sub_module_dir)\n",
    "from modules.models import RetinaFaceModel\n",
    "from modules.utils import pad_input_image, recover_pad_output, load_yaml\n",
    "\n",
    "# Python default libraries\n",
    "from pathlib import Path\n",
    "import time\n",
    "import base64\n",
    "\n",
    "# processing images\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "\n",
    "# visualising the data \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# masked or not classification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Disable TF warnings (Disabling the warnings is not a good practice but we disable it to make this notebook prettier)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Max enabled image width is set as 850. If greater we will resize the input images \n",
    "BASEWIDTH = 850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Face Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baturayofluoglu/anaconda3/envs/model-zoo/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x153b46450>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retina_face_dir = Path(ROOT_DIR) / 'retinaface-tf2'\n",
    "retinaface_cfg_path = retina_face_dir / 'configs' / 'retinaface_mbv2.yaml'\n",
    "retinaface_cfg = load_yaml(retinaface_cfg_path)\n",
    "\n",
    "# define network\n",
    "model = RetinaFaceModel(retinaface_cfg, training=False, iou_th=0.4, score_th=0.5)\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint_dir = retina_face_dir / retinaface_cfg['sub_name']\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mask Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dir = Path(ROOT_DIR) / 'data' / 'classifier_model_weights'\n",
    "classifier = tf.keras.models.load_model(classifier_dir / 'best.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img):\n",
    "    # Resize image by keeping the aspect ratio if image witdth is greater than BASEWIDTH\n",
    "    if img.size[0] > BASEWIDTH:\n",
    "        wpercent = (BASEWIDTH / float(img.size[0]))\n",
    "        hsize = int((float(img.size[1])*float(wpercent)))\n",
    "        img = img.resize((BASEWIDTH,hsize), Image.ANTIALIAS)\n",
    "    return img\n",
    "\n",
    "def detect_faces(img_raw):\n",
    "    img = np.float32(img_raw.copy())\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img, pad_params = pad_input_image(img, max_steps=max(retinaface_cfg['steps']))\n",
    "    detected_faces = model(img[np.newaxis, ...]).numpy()\n",
    "    return recover_pad_output(detected_faces, pad_params)\n",
    "\n",
    "def get_detected_face_coordinates(detected_faces, img_width, img_height):\n",
    "    detected_face_coordinates = []\n",
    "    for detected_face in detected_faces: \n",
    "        x1 = int(detected_face[0] * img_width)\n",
    "        y1 = int(detected_face[1] * img_height)\n",
    "        x2 = int(detected_face[2] * img_width)\n",
    "        y2 = int(detected_face[3] * img_height)\n",
    "        detected_face_coordinates.append([x1, y1, x2, y2])\n",
    "    return detected_face_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function for Mask Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_faces(img_raw, face_coords):\n",
    "    classification_scores = []\n",
    "    # Iterate over detected face coordinates to find\n",
    "    for coords in face_coords:\n",
    "        x1, y1, x2, y2 = coords\n",
    "        cropped_face = img_raw.crop(coords)\n",
    "        cropped_face_np = np.float32(cropped_face)\n",
    "        img = cv2.cvtColor(cropped_face_np, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (112, 112)) \n",
    "        preprocessed_img = tf.keras.applications.mobilenet.preprocess_input(img)\n",
    "        preprocessed_img = preprocessed_img[np.newaxis, ...]\n",
    "        pred = classifier.predict(preprocessed_img)[0][0]\n",
    "        classification_scores.append(pred)\n",
    "    return classification_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate Output Image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image(img, face_coords, classified_face_scores, classification_labels):\n",
    "    pil_draw = ImageDraw.Draw(img)\n",
    "    for idx, coords in enumerate(face_coords):\n",
    "        x1, y1, x2, y2 = coords\n",
    "        label = classification_labels[idx]\n",
    "        color = 'green' if label == 'masked' else 'red'\n",
    "        display_str = \"{}: {:.2f}\".format(label, classified_face_scores[idx])\n",
    "\n",
    "        # Draw rectangle for detected face\n",
    "        pil_draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "        \n",
    "        # Draw label text box\n",
    "        # portion of image width you want text width to be\n",
    "        img_fraction = 0.2\n",
    "        font_size = 5  # starting font size\n",
    "        \n",
    "        font = ImageFont.truetype(os.path.join(ROOT_DIR, \"arial.ttf\"), font_size)\n",
    "        image_size = img.size[0]\n",
    "        \n",
    "        while font.getsize(display_str)[0] < img_fraction * image_size:\n",
    "            # iterate until the text size is just larger than the criteria\n",
    "            font_size += 1\n",
    "            font = ImageFont.truetype(os.path.join(ROOT_DIR, \"arial.ttf\"), font_size)\n",
    "\n",
    "        # Find coordinates of bounding text box\n",
    "        w, h = font.getsize(display_str)\n",
    "        pil_draw.rectangle([x1, y1, x1 + w, y1 + h], fill=color)\n",
    "        pil_draw.text((x1, y1), display_str, font=font)\n",
    "    return img\n",
    "\n",
    "def convert_pil_to_base64(annotated_image, image_type):\n",
    "    buffered = BytesIO()\n",
    "    if image_type == 'jpg':\n",
    "        annotated_image.save(buffered, format='jpeg')\n",
    "    else:\n",
    "        annotated_image.save(buffered, format=image_type)\n",
    "    annotated_image_base64 = base64.b64encode(buffered.getvalue())\n",
    "    return annotated_image_base64.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint Function To Deploy our Model in Dploy.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @dploy endpoint predict\n",
    "def predict_masked_faces(body):\n",
    "    base64_image = body['image'].encode('utf-8')\n",
    "    image_type = body['type']\n",
    "    \n",
    "    # Convert image from base64 to PIL Image and resize it to improve the performance\n",
    "    img_raw = Image.open(BytesIO(base64.b64decode(base64_image)))\n",
    "    img_raw = resize_image(img_raw)\n",
    "\n",
    "    # Detect face coordinates from the raw image\n",
    "    detected_faces = detect_faces(img_raw)\n",
    "    print(detected_faces)\n",
    "    # Get detected face coordinates\n",
    "    img_width, img_height  = img_raw.size\n",
    "    face_coords = get_detected_face_coordinates(detected_faces, img_width, img_height)\n",
    "    \n",
    "    # Classify detected faces whether they have a mask or not\n",
    "    classified_face_scores = classify_faces(img_raw, face_coords)\n",
    "    \n",
    "    # Find labels\n",
    "    classification_labels = np.where(np.array(classified_face_scores) > 0.5, 'masked', 'not masked').tolist()\n",
    "    \n",
    "    # Annotate base image with detected faces and mask classification\n",
    "    annotated_image = annotate_image(img_raw, face_coords, classified_face_scores, classification_labels)\n",
    "    \n",
    "    # Convert PIL image to base64\n",
    "    annotated_image_base_64 = convert_pil_to_base64(annotated_image, image_type)\n",
    "    \n",
    "    return {\n",
    "            'detected_face_coordinates': face_coords,\n",
    "            'detected_mask_scores': classified_face_scores,\n",
    "            'detected_face_labels': classification_labels,\n",
    "            'annotated_image': annotated_image_base_64\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Entire Pipeline with Endpoint Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/baturayofluoglu/Workspace/face-mask-detection/1.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4828f24de3ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1.jpeg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     body = {\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/baturayofluoglu/Workspace/face-mask-detection/1.jpeg'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "_time = time.time()\n",
    "with open(os.path.join(ROOT_DIR, \"1.jpeg\"), \"rb\") as img_file:\n",
    "    body = {\n",
    "        'image': base64.b64encode(img_file.read()).decode('utf-8'),\n",
    "        'type': 'jpeg'\n",
    "    }\n",
    "with open('./test_data.json', 'w') as f:\n",
    "    json.dump(body, f)\n",
    "\n",
    "with open('./test_data.json', 'r') as f:\n",
    "    request_body = json.load(f)\n",
    "    response = predict_masked_faces(request_body)\n",
    "\n",
    "#Image.open(BytesIO(base64.b64decode(response['image']))).show()\n",
    "print(time.time() - _time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-zoo",
   "language": "python",
   "name": "model-zoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
