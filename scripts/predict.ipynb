{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sub_module_dir = os.path.join(ROOT_DIR, 'retinaface-tf2')\n",
    "if sub_module_dir not in sys.path: # add retinaface-tf2 repo to PATH\n",
    "    sys.path.append(sub_module_dir)\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import base64\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from modules.models import RetinaFaceModel\n",
    "from modules.utils import pad_input_image, recover_pad_output, load_yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "\n",
    "# Disable TF warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "BASEWIDTH = 850\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Face Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/baturayofluoglu/Workspace/face-mask-detection/retinaface-tf2\n",
      "/Users/baturayofluoglu/Workspace/face-mask-detection/retinaface-tf2/configs/retinaface_mbv2.yaml\n",
      "/Users/baturayofluoglu/Workspace/face-mask-detection/retinaface-tf2/retinaface_mbv2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15f0a94d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retina_face_dir = Path(ROOT_DIR) / 'retinaface-tf2'\n",
    "retinaface_cfg_path = retina_face_dir / 'configs' / 'retinaface_mbv2.yaml'\n",
    "retinaface_cfg = load_yaml(retinaface_cfg_path)\n",
    "\n",
    "# define network\n",
    "model = RetinaFaceModel(retinaface_cfg, training=False, iou_th=0.4, score_th=0.5)\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint_dir = retina_face_dir / retinaface_cfg['sub_name']\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mask Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dir = Path(ROOT_DIR) / 'data' / 'classifier_model_weights'\n",
    "classifier = tf.keras.models.load_model(classifier_dir / 'best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img):\n",
    "    # Resize image by keeping the aspect ratio if image witdth is greater than BASEWIDTH\n",
    "    if img.size[0] > BASEWIDTH:\n",
    "        wpercent = (BASEWIDTH / float(img.size[0]))\n",
    "        hsize = int((float(img.size[1])*float(wpercent)))\n",
    "        img = img.resize((BASEWIDTH,hsize), Image.ANTIALIAS)\n",
    "    return img\n",
    "\n",
    "def detect_faces(img_raw):\n",
    "    img = np.float32(img_raw.copy())\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img, pad_params = pad_input_image(img, max_steps=max(retinaface_cfg['steps']))\n",
    "    detected_faces = model(img[np.newaxis, ...]).numpy()\n",
    "    return recover_pad_output(detected_faces, pad_params)\n",
    "\n",
    "def get_detected_face_coordinates(detected_faces, img_width, img_height):\n",
    "    detected_face_coordinates = []\n",
    "    for detected_face in detected_faces: \n",
    "        x1 = int(detected_face[0] * img_width)\n",
    "        y1 = int(detected_face[1] * img_height)\n",
    "        x2 = int(detected_face[2] * img_width)\n",
    "        y2 = int(detected_face[3] * img_height)\n",
    "        detected_face_coordinates.append([x1, y1, x2, y2])\n",
    "    return detected_face_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_faces(img_raw, face_coords):\n",
    "    classification_scores = []\n",
    "    # Iterate over detected face coordinates to find\n",
    "    for coords in face_coords:\n",
    "        x1, y1, x2, y2 = coords\n",
    "        cropped_face = img_raw.crop(coords)\n",
    "        cropped_face_np = np.float32(cropped_face)\n",
    "        img = cv2.cvtColor(cropped_face_np, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (112, 112)) \n",
    "        preprocessed_img = tf.keras.applications.mobilenet.preprocess_input(img)\n",
    "        preprocessed_img = preprocessed_img[np.newaxis, ...]\n",
    "        pred = classifier.predict(preprocessed_img)[0][0]\n",
    "        classification_scores.append(pred)\n",
    "    return classification_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image(img, face_coords, classified_face_scores, classification_labels):\n",
    "    pil_draw = ImageDraw.Draw(img)\n",
    "    for idx, coords in enumerate(face_coords):\n",
    "        x1, y1, x2, y2 = coords\n",
    "        label = classification_labels[idx]\n",
    "        color = 'green' if label == 'masked' else 'red'\n",
    "        display_str = \"{}: {:.2f}\".format(label, classified_face_scores[idx])\n",
    "\n",
    "        # Draw rectangle for detected face\n",
    "        pil_draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "        \n",
    "        # Draw label text box\n",
    "        # portion of image width you want text width to be\n",
    "        img_fraction = 0.2\n",
    "        font_size = 5  # starting font size\n",
    "        \n",
    "        font = ImageFont.truetype(os.path.join(ROOT_DIR, \"arial.ttf\"), font_size)\n",
    "        image_size = img.size[0]\n",
    "        \n",
    "        while font.getsize(display_str)[0] < img_fraction * image_size:\n",
    "            # iterate until the text size is just larger than the criteria\n",
    "            font_size += 1\n",
    "            font = ImageFont.truetype(os.path.join(ROOT_DIR, \"arial.ttf\"), font_size)\n",
    "\n",
    "        # Find coordinates of bounding text box\n",
    "        w, h = font.getsize(display_str)\n",
    "        pil_draw.rectangle([x1, y1, x1 + w, y1 + h], fill=color)\n",
    "        pil_draw.text((x1, y1), display_str, font=font)\n",
    "    return img\n",
    "\n",
    "def convert_pil_to_base64(annotated_image, image_type):\n",
    "    buffered = BytesIO()\n",
    "    if image_type == 'jpg':\n",
    "        annotated_image.save(buffered, format='jpeg')\n",
    "    else:\n",
    "        annotated_image.save(buffered, format=image_type)\n",
    "    annotated_image_base64 = base64.b64encode(buffered.getvalue())\n",
    "    return annotated_image_base64.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @dploy endpoint predict\n",
    "def predict_masked_faces(body):\n",
    "    base64_image = body['image'].encode('utf-8')\n",
    "    image_type = body['type']\n",
    "    \n",
    "    # Convert image from base64 to PIL Image and resize it to improve the performance\n",
    "    img_raw = Image.open(BytesIO(base64.b64decode(base64_image)))\n",
    "    img_raw = resize_image(img_raw)\n",
    "\n",
    "    # Detect face coordinates from the raw image\n",
    "    detected_faces = detect_faces(img_raw)\n",
    "    print(detected_faces)\n",
    "    # Get detected face coordinates\n",
    "    img_width, img_height  = img_raw.size\n",
    "    face_coords = get_detected_face_coordinates(detected_faces, img_width, img_height)\n",
    "    \n",
    "    # Classify detected faces whether they have a mask or not\n",
    "    classified_face_scores = classify_faces(img_raw, face_coords)\n",
    "    \n",
    "    # Find labels\n",
    "    classification_labels = np.where(np.array(classified_face_scores) > 0.5, 'masked', 'not masked').tolist()\n",
    "    \n",
    "    # Annotate base image with detected faces and mask classification\n",
    "    annotated_image = annotate_image(img_raw, face_coords, classified_face_scores, classification_labels)\n",
    "    \n",
    "    # Convert PIL image to base64\n",
    "    annotated_image_base_64 = convert_pil_to_base64(annotated_image, image_type)\n",
    "    \n",
    "    return {\n",
    "            'detected_face_coordinates': face_coords,\n",
    "            'detected_mask_scores': classified_face_scores,\n",
    "            'detected_face_labels': classification_labels,\n",
    "            'annotated_image': annotated_image_base_64\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38385132 0.12743628 0.5838287  0.518403   0.43789974 0.29725102\n",
      "  0.5258882  0.29297885 0.4824697  0.3529638  0.4508822  0.42788082\n",
      "  0.5200763  0.42299545 1.         0.9999862 ]\n",
      " [0.0960518  0.7258081  0.3056761  0.96894056 0.15428889 0.9007342\n",
      "  0.18773465 0.9013555  0.15501799 0.9637476  0.19403099 0.9545578\n",
      "  0.20730095 0.9709965  1.         0.58616716]]\n",
      "1.1999261379241943\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "_time = time.time()\n",
    "with open(os.path.join(ROOT_DIR, \"1.jpg\"), \"rb\") as img_file:\n",
    "    body = {\n",
    "        'image': base64.b64encode(img_file.read()).decode('utf-8'),\n",
    "        'type': 'jpeg'\n",
    "    }\n",
    "with open('./test_data.json', 'w') as f:\n",
    "    json.dump(body, f)\n",
    "\n",
    "with open('./test_data.json', 'r') as f:\n",
    "    request_body = json.load(f)\n",
    "    response = predict_masked_faces(request_body)\n",
    "\n",
    "#Image.open(BytesIO(base64.b64decode(response['image']))).show()\n",
    "print(time.time() - _time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-zoo",
   "language": "python",
   "name": "model-zoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
