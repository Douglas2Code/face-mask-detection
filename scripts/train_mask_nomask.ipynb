{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the mask classification model\n",
    "\n",
    "We train a model with a MobileNetV1 base with ImageNet pre-trained weights to distinguish between masked and non-masked faces.  \n",
    "The MobileNetV1 base is followed by a fully connected layer and a layer with sigmoid activiation for classification.  \n",
    "Only the final 4 layers of the model are finetuned, other weights are frozen.\n",
    "\n",
    "### Training\n",
    "\n",
    "Only unmasked and artificially masked data based on VGGFace2 (see prep_data.ipynb) are used to train the model.  \n",
    "\n",
    "### Validation\n",
    "We work with two separate validation set: \n",
    "  1. One containing masked and artificially masked data based on VGGFace2\n",
    "  2. One containing unmasked and real masked data from our own collected dataset.  \n",
    "  \n",
    "This helps us to distinguish between performance on artificial and real data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "import keras \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn import metrics \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (112,112)\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "n_epochs = 2 \n",
    "\n",
    "root_dir = os.path.dirname(os.path.abspath(os.curdir))\n",
    "data_dir = Path(root_dir) / 'data'\n",
    "\n",
    "model_dir = data_dir / 'classifier_model_weights'\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "performance_plots_dir = Path('img')\n",
    "performance_plots_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/validation/test generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unmasked and artificially masked images from VGGFace2 are divided into a training and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3949 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_datagen.flow_from_directory(str(data_dir / 'train'),\n",
    "                                                    target_size=target_size,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='binary',\n",
    "                                                    classes=['not_masked', 'masked'],\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 986 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen_artificial = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_generator_artificial = val_datagen_artificial.flow_from_directory(str(data_dir / 'validation' / 'artificial'),\n",
    "                                                    target_size=target_size,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='binary',\n",
    "                                                    classes=['not_masked', 'masked'],\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a separate validation generator to keep track of performance on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 262 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen_real = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_generator_real = val_datagen_real.flow_from_directory(str(data_dir / 'validation' / 'real'),\n",
    "                                                    target_size=target_size,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='binary',\n",
    "                                                    classes=['not_masked', 'masked'],\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "MobileNet base with 1 fully connected layers on top and one final layer with sigmoid activation that outputs predictions.  \n",
    "Only the last layers are trained, others are frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toon/mask_env_37/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNet(weights='imagenet',include_top=False, input_shape=(target_size[0],target_size[1],3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128,activation='relu')(x) \n",
    "preds = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "for layer in model.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper callback to evaluate performance on real data after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetAccuracy(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    We want to monitor accuracy in the validation set separately for real and artificial face masks.\n",
    "    This callback will print this to the output, and store the values for each epoch.\n",
    "    \n",
    "    It also stores the best model (according to validation accuracy on the masked faces) to disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, real_val_gen=None):\n",
    "        self.real_val_gen = real_val_gen\n",
    "        self.cur_best_acc = 0\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        pred = self.model.predict(self.real_val_gen)\n",
    "        bin_pred = [x > 0.5 for x in pred]\n",
    "        real_acc = metrics.accuracy_score(self.real_val_gen.classes, bin_pred)\n",
    "        \n",
    "        print(f\"Accuracy on the real validation set: {real_acc:.2f}\")\n",
    "        \n",
    "        if real_acc > self.cur_best_acc:\n",
    "            self.model.save(model_dir / 'best.h5')\n",
    "            self.cur_best_acc = real_acc\n",
    "          \n",
    "subset_acc = SubsetAccuracy(val_generator_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "step_size_train = train_generator.n//train_generator.batch_size\n",
    "step_size_val = val_generator_artificial.n//val_generator_artificial.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "123/123 [==============================] - 24s 192ms/step - loss: 0.4407 - accuracy: 0.8953 - val_loss: 0.0859 - val_accuracy: 0.9865\n",
      "Accuracy on the real validation set: 0.89\n",
      "Epoch 2/2\n",
      " 70/123 [================>.............] - ETA: 8s - loss: 0.0566 - accuracy: 0.9817"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=train_generator, steps_per_epoch=step_size_train, epochs=n_epochs, validation_data=val_generator_artificial, validation_steps=step_size_val, callbacks=[subset_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on real validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(model_dir / 'best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = best_model.predict_generator(val_generator_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    From: https://scikit-learn.org/0.18/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on real validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_bin = [x[0] > 0.5 for x in val_pred]\n",
    "\n",
    "acc = metrics.accuracy_score(val_generator_real.classes, val_pred_bin)\n",
    "print(f\"Accuracy = {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on real validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(val_generator_real.classes, val_pred_bin)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, ['not masked', 'masked'])\n",
    "plt.savefig(performance_plots_dir / 'confusion.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC ruve on real validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thr = metrics.roc_curve(val_generator_real.classes, val_pred)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', size=15)\n",
    "plt.ylabel('True Positive Rate', size=15)\n",
    "plt.title(f'AUC = {auc:.3f}', size=15)\n",
    "plt.savefig(performance_plots_dir / 'roc.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the mistakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_plot = 16\n",
    "\n",
    "mistakes = []\n",
    "correct = []\n",
    "for fn, true_label, pred_label in zip(val_generator_real.filenames, val_generator_real.classes, val_pred_bin):\n",
    "    if true_label != pred_label:\n",
    "        mistakes.append(data_dir / 'validation' / 'real'/ fn)\n",
    "    else: \n",
    "        correct.append(data_dir / 'validation' / 'real'/ fn)\n",
    "        \n",
    "nrow = np.ceil(np.sqrt(n_to_plot))\n",
    "ncol = nrow\n",
    "\n",
    "to_plot = random.sample(correct,n_to_plot)\n",
    "plt.figure(figsize=(10,10))\n",
    "for idx, x in enumerate(to_plot):\n",
    "    plt.subplot(nrow, ncol, idx+1)\n",
    "    img = cv2.imread(str(x))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample of images classified correctly', size=20, y=0.92)\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()\n",
    "\n",
    "to_plot = random.sample(correct,n_to_plot)\n",
    "plt.figure(figsize=(10,10))\n",
    "for idx, x in enumerate(to_plot):\n",
    "    plt.subplot(nrow, ncol, idx+1)\n",
    "    img = cv2.imread(str(x))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample of images classified wrongly', size=20, y=0.92)\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
