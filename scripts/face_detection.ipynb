{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection on masked images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sub_module_dir = os.path.abspath(os.path.join('../retinaface-tf2'))\n",
    "if sub_module_dir not in sys.path: # add retinaface-tf2 repo to PATH\n",
    "    sys.path.append(sub_module_dir)\n",
    "    \n",
    "from pathlib import Path\n",
    "from modules.models import RetinaFaceModel\n",
    "from modules.utils import pad_input_image, recover_pad_output, load_yaml\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath(os.curdir))\n",
    "DATA_DIR = Path(ROOT_DIR) / 'data'\n",
    "RAW_IMAGES_DIR = DATA_DIR / 'raw_images'\n",
    "\n",
    "REAL_VALID_DIR = DATA_DIR / 'validation' / 'real'\n",
    "REAL_VALID_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "REAL_TEST_DIR = DATA_DIR / 'test'\n",
    "REAL_TEST_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DETECTOR_RESULT_DIR = DATA_DIR / 'detector_results'\n",
    "DETECTOR_RESULT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DETECTOR_BBOX_ANNOTATIONS = DATA_DIR / 'detector_annotations.csv'\n",
    "\n",
    "GROUND_TRUTH_ANNOTATIONS = DATA_DIR / 'image_annotations.csv'\n",
    "\n",
    "# Add retinaface-tf2 submodule to PATH\n",
    "RETINAFACE_DIR = os.path.join(ROOT_DIR, 'retinaface-tf2')\n",
    "if RETINAFACE_DIR not in sys.path: # add retinaface-tf2 repo to PATH\n",
    "    sys.path.append(RETINAFACE_DIR)\n",
    "RETINAFACE_CFG_PATH = Path(RETINAFACE_DIR) / 'configs' / 'retinaface_mbv2.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toon/mask_env_37/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb3586ed0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retinaface_cfg = load_yaml(RETINAFACE_CFG_PATH)\n",
    "\n",
    "# define network\n",
    "model = RetinaFaceModel(retinaface_cfg, training=False, iou_th=0.4, score_th=0.5)\n",
    "\n",
    "# load checkpoint\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "checkpoint_dir = os.path.join('../retinaface-tf2', retinaface_cfg['sub_name'])\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_raw_img = list(sorted(RAW_IMAGES_DIR.glob('*.jpg')))\n",
    "\n",
    "random.seed(2362)\n",
    "random.shuffle(real_raw_img)\n",
    "\n",
    "VALID_PCT_REAL_IMAGES = 0.5     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_new/face-mask-detection/data/raw_images/131.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_new/face-mask-detection/data/raw_images/46.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_new/face-mask-detection/data/raw_images/192.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_new/face-mask-detection/data/raw_images/101.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_new/face-mask-detection/data/raw_images/245.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_new/face-mask-detection/data/raw_images/130.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_new/face-mask-detection/data/raw_images/149.jpg\n",
      "Detector extracted a face with 0 rows or columns, skipping this one for /home/toon/faces_new/face-mask-detection/data/raw_images/133.jpg\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "detector_bbox = pd.DataFrame(columns=['img','x','y','w','h'])\n",
    "\n",
    "for idx, fn in enumerate(real_raw_img):\n",
    "    \n",
    "    destination_dir = REAL_VALID_DIR if idx <= int(VALID_PCT_REAL_IMAGES*len(real_raw_img)) else REAL_TEST_DIR\n",
    "    \n",
    "    if fn.name == '157.jpg':\n",
    "        # this one crashes the kernel, don't know why..\n",
    "        continue\n",
    "        \n",
    "    img_raw = cv2.imread(str(fn))\n",
    "    img_height, img_width, _ = img_raw.shape\n",
    "    img = np.float32(img_raw.copy())\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img, pad_params = pad_input_image(img, max_steps=max(retinaface_cfg['steps']))\n",
    "    \n",
    "    outputs = model(img[np.newaxis, ...]).numpy()\n",
    "    outputs = recover_pad_output(outputs, pad_params)\n",
    "      \n",
    "    for prior_index in range(len(outputs)):\n",
    "        ann = outputs[prior_index]\n",
    "        x1, y1, x2, y2 = int(ann[0] * img_width), int(ann[1] * img_height), \\\n",
    "                         int(ann[2] * img_width), int(ann[3] * img_height)\n",
    "        \n",
    "        detector_bbox.loc[len(detector_bbox)] = [fn.name,x1,y1,x2-x1,y2-y1]\n",
    "\n",
    "        \n",
    "        rect = plt.Rectangle((x1,y1),x2-x1,y2-y1,linewidth=3,edgecolor='green',facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        cropped_face = img[y1:y2, x1:x2]\n",
    "        \n",
    "        if cropped_face.shape[0] == 0 or cropped_face.shape[1] == 0:\n",
    "            print(f\"Detector extracted a face with 0 rows or columns, skipping this one for {fn}\")\n",
    "            continue\n",
    "            \n",
    "        output_fn = destination_dir / (fn.stem + '_' + str(prior_index) + '.jpg')\n",
    "        cv2.imwrite(str(output_fn), cv2.cvtColor(cropped_face, cv2.COLOR_RGB2BGR))     \n",
    "    \n",
    "detector_bbox.to_csv(DETECTOR_BBOX_ANNOTATIONS, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate wrt ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    '''\n",
    "    from https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "    '''\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-1b3edf9b78a8>, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-1b3edf9b78a8>\"\u001b[0;36m, line \u001b[0;32m71\u001b[0m\n\u001b[0;31m    fp += n_duplicates\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "ground_truth = pd.read_csv(GROUND_TRUTH_ANNOTATIONS)\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "correct = []\n",
    "mistakes = []\n",
    "    \n",
    "for idx, img_fn in enumerate(real_raw_img):\n",
    "    img = cv2.imread(str(img_fn))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(img/255)\n",
    "    \n",
    "    ground_truth_fn = ground_truth[ground_truth.img == img_fn.name]\n",
    "    \n",
    "    gt_boxes = []\n",
    "    for row_idx in range(ground_truth_fn.shape[0]):\n",
    "        ground_truth_annotation = ground_truth_fn.iloc[row_idx]\n",
    "\n",
    "        x = ground_truth_annotation.x.item()\n",
    "        y = ground_truth_annotation.y.item()\n",
    "        w = ground_truth_annotation.w.item()\n",
    "        h = ground_truth_annotation.h.item()\n",
    "        \n",
    "        gt_boxes.append((x,y,x+w,y+h))\n",
    "        \n",
    "        rect = plt.Rectangle((x,y),w,h,linewidth=3,edgecolor='green',facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "    bbox_fn = detector_bbox[detector_bbox.img == img_fn.name]\n",
    "   \n",
    "    pred_boxes = []\n",
    "    for row_idx in range(bbox_fn.shape[0]):\n",
    "        pred_annotation = bbox_fn.iloc[row_idx]\n",
    "\n",
    "        x = pred_annotation.x\n",
    "        y = pred_annotation.y\n",
    "        w = pred_annotation.w\n",
    "        h = pred_annotation.h\n",
    "        \n",
    "        pred_boxes.append((x,y,x+w,y+h))\n",
    "        \n",
    "        rect = plt.Rectangle((x,y),w,h,linewidth=3,edgecolor='red',facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.savefig(DETECTOR_RESULT_DIR / img_fn.name, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "    fn_prev = fn\n",
    "    fp_prev = fp\n",
    "    \n",
    "    # first we check for each ground truth bounding box whether it was detected\n",
    "    for gt_box in gt_boxes:\n",
    "        times_found = 0\n",
    "        for pred_box in pred_boxes: \n",
    "            iou = bb_intersection_over_union(gt_box, pred_box)\n",
    "            if iou >= 0.5:\n",
    "                times_found += 1\n",
    "        \n",
    "        if times_found > 0:\n",
    "            # the face was detected at least once, so it is a true positive\n",
    "            tp += 1\n",
    "        \n",
    "            # if it is detected more than once, we add the duplicate detections to the false positives\n",
    "            n_duplicates = times_found - 1\n",
    "            if n_duplicates > 0:\n",
    "                fp += n_duplicates\n",
    "        \n",
    "        if times_found == 0:\n",
    "            fn += 1\n",
    "        \n",
    "    # we also have to check for detected bounding boxes that do not overlap with any ground truth bounding box,\n",
    "    # as these are counted as false negatives\n",
    "    for pred_box in pred_boxes:\n",
    "        found = False\n",
    "        for gt_box in gt_boxes:\n",
    "            iou = bb_intersection_over_union(gt_box, pred_box)\n",
    "            if iou > 0.5:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            fp += 1\n",
    "            \n",
    "    if fn_prev != fn or fp_prev != fp:\n",
    "        mistakes.append(DETECTOR_RESULT_DIR / img_fn.name)\n",
    "    else:\n",
    "        correct.append(DETECTOR_RESULT_DIR / img_fn.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of true positives: {tp}\")\n",
    "print(f\"Number of false positives: {fp}\")\n",
    "print(f\"Number of false negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_plot = 16\n",
    "to_plot = random.sample(correct,n_to_plot)\n",
    "\n",
    "nrow = np.ceil(np.sqrt(n_to_plot))\n",
    "ncol = nrow\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, x in enumerate(to_plot):\n",
    "    plt.subplot(nrow, ncol, idx+1)\n",
    "    img = cv2.imread(str(x))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample of images with all faces detected correctly', size=20, y=0.92)\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_plot = 16\n",
    "to_plot = random.sample(mistakes,n_to_plot)\n",
    "\n",
    "nrow = np.ceil(np.sqrt(n_to_plot))\n",
    "ncol = nrow\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, x in enumerate(to_plot):\n",
    "    plt.subplot(nrow, ncol, idx+1)\n",
    "    img = cv2.imread(str(x))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample of images with mistakes in face detection', size=20, y=0.92)\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
