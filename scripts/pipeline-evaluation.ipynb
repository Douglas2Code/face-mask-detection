{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the unmasked face detection pipeline\n",
    "\n",
    "In this notebook we evaluate the combination of the face detector and mask/no_mask classifier.  \n",
    "\n",
    "For each image in the validation/test set, we do the following:  \n",
    "  1. Fetch the cropped faces that are the result of the RetinaFace face detector (see face-detection-evaluation.ipynb)\n",
    "  2. Apply the classification model to these cropped faces to obtain a mask/no_mask prediction\n",
    "  3. Match the detected faces with the ground truth ones and compare the predicted label to the ground truth label.\n",
    "     A predicted face bounding box matches a ground truth bounding box if their intersection over union (IoU) > 0.5.  \n",
    "  \n",
    "Based on these matched ground truth labels and predictions, we can compute performance statistics (confusion matrix, ROC, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from sklearn import metrics \n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import itertools\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (112,112)\n",
    "\n",
    "root_dir = os.path.dirname(os.path.abspath(os.curdir))\n",
    "data_dir = Path(root_dir) / 'data'\n",
    "\n",
    "model_dir = data_dir / 'classifier_model_weights'\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "real_validation_dir = data_dir / 'validation' / 'real'\n",
    "real_test_dir = data_dir / 'test'\n",
    "\n",
    "# ROC curves will be stored here\n",
    "performance_plots_dir = Path('img')\n",
    "performance_plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# plots that compare pipeline predictions to ground truth will be stored here\n",
    "prediction_plot_dir = Path('data_dir/overall_model_evaluation/predictions')\n",
    "plot_dir_correct = prediction_plot_dir / 'all_correct'\n",
    "plot_dir_correct.mkdir(exist_ok=True, parents=True)\n",
    "plot_dir_mistake = prediction_plot_dir / 'mistake'\n",
    "plot_dir_mistake.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation and test generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a validation generator and test generator for real masked and unmasked data. \n",
    "The input images to the mask/no_mask classifier are the output of the face detector (generated previously in face_detection.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need both the ground truth and detector bounding boxes to evaluate the IoU \n",
    "detector_bboxes = pd.read_csv(data_dir / 'detector_annotations.csv')\n",
    "ground_truth_boxes = pd.read_csv(data_dir / 'test_validation_metadata.csv')\n",
    "ground_truth_boxes['img_name'] = ground_truth_boxes.raw_img_dir.apply(lambda x: Path(x).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rows = ground_truth_boxes[ground_truth_boxes.cropped_img_dir.str.contains(\"validation\")]\n",
    "# images can occur multiple times as rows in the test_validation_metadata.csv file as they can contain multiple faes\n",
    "# here we need to have each original filename only once\n",
    "val_fns = list(set(val_rows.img_name))\n",
    "\n",
    "val_frame_real = pd.DataFrame(columns=['original_image','bbox_id','x','y','w','h','class','image'])\n",
    "for fn in val_fns:\n",
    "    detector_bboxes_fn = detector_bboxes[detector_bboxes.img == fn]\n",
    "    for d_idx, d_row in detector_bboxes_fn.iterrows():\n",
    "        val_frame_real.loc[len(val_frame_real)] = [fn,d_row.bbox_id,d_row.x, d_row.y, d_row.w, d_row.h, 'none', d_row.crop_fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows = ground_truth_boxes[ground_truth_boxes.cropped_img_dir.str.contains(\"test\")]\n",
    "test_fns = list(set(test_rows.img_name))\n",
    "\n",
    "test_frame_real = pd.DataFrame(columns=['original_image','bbox_id','x','y','w','h','class','image'])\n",
    "for fn in test_fns:\n",
    "    detector_bboxes_fn = detector_bboxes[detector_bboxes.img == fn]\n",
    "    for d_idx, d_row in detector_bboxes_fn.iterrows():\n",
    "        test_frame_real.loc[len(test_frame_real)] = [fn,d_row.bbox_id,d_row.x, d_row.y, d_row.w, d_row.h, 'none', d_row.crop_fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 255 validated image filenames belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen_real = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_frame_real.image = val_frame_real.image.astype(str)\n",
    "val_generator_real = val_datagen_real.flow_from_dataframe(val_frame_real, \n",
    "                                                shuffle=False,\n",
    "                                                target_size=target_size,\n",
    "                                                x_col='image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256 validated image filenames belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen_real = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_frame_real.image = test_frame_real.image.astype(str)\n",
    "test_generator_real = test_datagen_real.flow_from_dataframe(test_frame_real, \n",
    "                                                shuffle=False,\n",
    "                                                target_size=target_size,\n",
    "                                                x_col='image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation code\n",
    "\n",
    "The functions below implement the evaluation process outlined at the top of this notebook (i.e. matching detector bounding boxes to ground truth and computing statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Computes the IoU between two bounding boxes\n",
    "    from https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "    \"\"\"\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_gt_box_with_pred_one_image(gt_box, pred_boxes, pred_scores, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    For a single ground truth bounding box gt_box, finds the predicted bounding boxes that overlap with IoU > iou_threshold,\n",
    "    and returns the corresponding predictions.\n",
    "    \n",
    "    :param gt_box: the ground truth boudning box for which to find overlapping predicted boxes\n",
    "    :param pred_boxes: the predicted bounding boxes that could be matched with gt_box\n",
    "    :param pred_scores: prediction scores corresponding to pred_boxes\n",
    "    :param iou_threshold: threshold that defines when ground truth box matches with predicted box\n",
    "    :return matches: a list of scores, one for each predicted bounding box that matched gt_box\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for pred_box, pred_score in zip(pred_boxes, pred_scores): \n",
    "        iou = bb_intersection_over_union(gt_box, pred_box)\n",
    "        if iou > iou_threshold:\n",
    "            matches.append(pred_score)\n",
    "    return matches\n",
    "\n",
    "def match_gt_boxes_with_pred_boxes(gt_boxes, gt_labels, pred_boxes, pred_scores):\n",
    "    \"\"\"\n",
    "    Matches ground truth bounding boxex and labels to predicted bounding boxes and scores for a single image.\n",
    "    \n",
    "    :param gt_boxes:  a list of (x1,y1,x2,y2) tuples that describe the ground truth bounding boxes for an image\n",
    "    :param gt_labels: the list of ground truth labels that correspond to gt_boxes\n",
    "    :param pred_boxes: a list of (x1,y1,x2,y2) tuples that describe the predicted bounding boxes for an image\n",
    "    :param pred_scores: the list of model predictions that correspond to pred_boxes\n",
    "    :return matched_gt: ground truth labels for ground truth face boxes\n",
    "    :return matched_pred: prediction scores for detected bounding boxes matched to ground truth boxes\n",
    "                         if none of the predicted bounding boxes matches a particular ground truth bounding box,\n",
    "                         we include None as a prediction\n",
    "    \"\"\"\n",
    "    matched_gt = []\n",
    "    matched_pred = []\n",
    "    \n",
    "    for gt_box, gt_label in zip(gt_boxes, gt_labels):\n",
    "        matched_gt.append(gt_label)\n",
    "        matched_scores = match_gt_box_with_pred_one_image(gt_box, pred_boxes, pred_scores)\n",
    "        if len(matched_scores) == 0:\n",
    "            # this face is not found by the detector\n",
    "            matched_pred.append(None)\n",
    "        elif len(matched_scores) == 1:\n",
    "            matched_pred.append(matched_scores[0])\n",
    "        else:\n",
    "            # there are multiple predictions, we take the lowest value (if there's an unmasked face, we definitely want to predict it)\n",
    "            # not that this happens very rarely\n",
    "            matched_pred.append(min(matched_scores)) \n",
    "        \n",
    "    return matched_gt, matched_pred\n",
    "\n",
    "def collect_preds_for_boxes_that_dont_match_gt(gt_boxes, pred_boxes, pred_scores):\n",
    "    \"\"\"\n",
    "    Collect the model predictions for boxes that are produced by the detector but that do not match a ground truth bounding box.\n",
    "    \n",
    "    :param gt_boxes: a list of (x1,y1,x2,y2) tuples that describe the ground truth bounding boxes for an image\n",
    "    :param pred_boxes: a list of (x1,y1,x2,y2) tuples that describe the predicted bounding boxes for an image\n",
    "    :param pred_scores: the list of model predictions that correspond to pred_boxes\n",
    "    :return corresponding_scores: the model predictions for the detector bounding boxes that do not match a ground truth bounding box \n",
    "    \"\"\"\n",
    "    corresponding_scores = []\n",
    "    for pred_box, pred_score in zip(pred_boxes, pred_scores):\n",
    "        found = False\n",
    "        for gt_box in gt_boxes:\n",
    "            if bb_intersection_over_union(gt_box, pred_box) > 0.5:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            corresponding_scores.append(pred_score)\n",
    "    return corresponding_scores\n",
    "\n",
    "def predict_and_match_to_gt(model, data_gen, data_df, plot=False):\n",
    "    \"\"\"\n",
    "    Apply a model to data generated by data_gen and match the resulting predictions to ground truth labels.\n",
    "    \n",
    "    :param model: the model to evaluate\n",
    "    :param data_gen: generates the validation data\n",
    "    :param data_df: stores the detected bounding boxes for the validation data \n",
    "    :param plot: if this is True an image will be produced in prediction_plot_dir with ground truth bounding boxes,\n",
    "            as well as predicted bounding boxes\n",
    "    :return matched_gt: list of ground truth labels \n",
    "    :return matched_pred: a list of of predictions matched to the ground truth labels \n",
    "               (i.e. the predictions for detected bounding boxes with IoU > 0.5 to the ground truth bounding box)\n",
    "               if there was no detected bounding box that matched a particular ground truth bounding box,\n",
    "               this list will contain None\n",
    "    :return scores_for_detector_boxes_not_found: a list of predictions for bounding boxes that were produced by the detector but \n",
    "               that don't correspond to any ground truth bounding box \n",
    "    \"\"\"\n",
    "    \n",
    "    pred = model.predict_generator(data_gen)\n",
    "    \n",
    "    # prepare the prediction dataframe for merging with data_df\n",
    "    pred_df = pd.DataFrame({'img' : data_gen.filenames,\n",
    "                            'pred' : np.squeeze(pred)})\n",
    "    \n",
    "    # from the cropped img name (e.g. 100_0.jpg) to the original image name (e.g. 100.jpg)\n",
    "    pred_df['original_image'] = pred_df.img.apply(lambda x: x.split('/')[-1].split('_')[0] + '.jpg') \n",
    "    \n",
    "    # extract the bbox id, e.g. cropped image 100_0.jpg corresponds to 0th extracted face bbox\n",
    "    pred_df['bbox_id'] = pred_df.img.apply(lambda x: int(x.split('/')[-1].split('_')[1].split('.')[0]))\n",
    "\n",
    "    # merging with data_df to get one dataframe with both the class predictions and detector bounding box coordinates\n",
    "    pred_df = pred_df.merge(data_df, left_on=['original_image', 'bbox_id'], right_on=['original_image', 'bbox_id'])\n",
    "    \n",
    "    # loop over all the validation images to compare ground-truth bounding boxes\n",
    "    # to predicted ones in that image\n",
    "    matched_gt = []\n",
    "    matched_pred = []\n",
    "    \n",
    "    scores_for_detector_boxes_not_found = []\n",
    "\n",
    "    val_img_ids = set(data_df.original_image)\n",
    "    for val_img_id in val_img_ids:\n",
    "        \n",
    "        # first collect ground truth bounding boxes and labels in a nicer format\n",
    "        img_ground_truth = ground_truth_boxes[ground_truth_boxes.img_name == val_img_id]\n",
    "        true_labels = []\n",
    "        true_boxes = []\n",
    "        for idx, row in img_ground_truth.iterrows():\n",
    "            true_labels.append(1 if row['class'] == 'masked' else 0)\n",
    "            true_boxes.append((row.x, row.y, row.x + row.w, row.y + row.h))\n",
    "\n",
    "        # do the same for the predictions\n",
    "        img_pred = pred_df[pred_df.original_image == val_img_id]\n",
    "        pred_labels = []\n",
    "        pred_boxes = []\n",
    "        pred_scores = []\n",
    "        for idx, row in img_pred.iterrows():\n",
    "            pred_boxes.append((row.x, row.y, row.x + row.w, row.y + row.h))\n",
    "            pred_scores.append(row.pred)\n",
    "            \n",
    "        if plot:\n",
    "            plot_img(data_dir / 'raw_images' / val_img_id, true_boxes, true_labels, pred_boxes, [x > 0.5 for x in pred_scores])\n",
    "       \n",
    "        # now match the ground truth bounding boxes and labels with the predicted ones\n",
    "        cur_matched_gt, cur_matched_pred = match_gt_boxes_with_pred_boxes(true_boxes, true_labels, pred_boxes, pred_scores)\n",
    "        \n",
    "        matched_gt.extend(cur_matched_gt)\n",
    "        matched_pred.extend(cur_matched_pred)\n",
    "        \n",
    "        cur_scores_for_boxes_without_gt = collect_preds_for_boxes_that_dont_match_gt(true_boxes, pred_boxes, pred_scores)\n",
    "        scores_for_detector_boxes_not_found.extend(cur_scores_for_boxes_without_gt)\n",
    "        \n",
    "    return matched_gt, matched_pred, scores_for_detector_boxes_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    From: https://scikit-learn.org/0.18/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_img(fn, true_boxes, true_labels, pred_boxes, pred_labels):\n",
    "    \"\"\"\n",
    "    Plot the results of the face detector and mask/no_mask classifier as well as ground-truth face boxes.\n",
    "    \n",
    "    :param fn: path to the image for which predictions are made\n",
    "    :param true_boxes: ground-truth face bounding boxes\n",
    "    :param true_label: ground-truth labels\n",
    "    :param pred_boxes: bounding boxes produced by face detector\n",
    "    :param pred_labels: predictions made by mask/no_mask classifier for bounding boxes produced by face detector\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(fn))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(img/255)\n",
    "    \n",
    "    for x1, y1, x2, y2 in true_boxes:\n",
    "        rect = plt.Rectangle((x1,y1),x2-x1,y2-y1,linewidth=3,edgecolor='black',facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "    for (x1, y1, x2, y2), predicted_mask in zip(pred_boxes, pred_labels):\n",
    "        rect = plt.Rectangle((x1,y1),x2-x1,y2-y1,linewidth=3, edgecolor='green' if predicted_mask else 'red',facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "   \n",
    "    output_dir = plot_dir_correct if true_labels == pred_labels else plot_dir_mistake\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_dir / fn.name, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(model_dir / 'best.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the validation set (only the part with real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-a4d9035db04d>:88: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'count_pred_boxes_that_dont_match_gt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2eecc0abcbeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatched_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatched_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_for_detector_boxes_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_and_match_to_gt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_generator_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_frame_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a4d9035db04d>\u001b[0m in \u001b[0;36mpredict_and_match_to_gt\u001b[0;34m(model, data_gen, data_df, plot)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mmatched_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_matched_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mcur_scores_for_boxes_without_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_pred_boxes_that_dont_match_gt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mscores_for_detector_boxes_not_found\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_scores_for_boxes_without_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_pred_boxes_that_dont_match_gt' is not defined"
     ]
    }
   ],
   "source": [
    "matched_gt, matched_scores, scores_for_detector_boxes_not_found = predict_and_match_to_gt(best_model,val_generator_real, val_frame_real, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{matched_scores.count(None)} ground truth face annotations were not detected by the face detector\")\n",
    "print(f\"{len(scores_for_detector_boxes_not_found)} boxes that were found by the face detector were not in the ground truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect ground truth and predictions for the detected bounding boxes\n",
    "matched_gt_not_none = [x for x,y in zip(matched_gt, matched_scores) if y is not None]\n",
    "matched_scores_not_none = [x for x in matched_scores if x is not None]\n",
    "matched_pred_not_none = [x > 0.5 for x in matched_scores_not_none]\n",
    "\n",
    "# collect ground truth and predictions for all ground truth bounding boxes, including 1 as a prediction if the ground-truth face was not detected\n",
    "matched_scores_none_replaced = [1 if x is None else x for x in matched_scores]\n",
    "matched_pred_none_replaced = [x > 0.5 for x in matched_scores_none_replaced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the mask/no_mask classifier on the ground-truth faces that have been detected by the detector: {metrics.accuracy_score(matched_gt_not_none,matched_pred_not_none):.2f}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(matched_gt_not_none, matched_pred_not_none)\n",
    "plot_confusion_matrix(cm, ['not masked', 'masked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thr = metrics.roc_curve(matched_gt, matched_scores_none_replaced)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', size=15)\n",
    "plt.ylabel('True Positive Rate', size=15)\n",
    "plt.title(f'ROC for complete pipeline\\nAUC = {auc:.3f}', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_gt, matched_scores, scores_for_detector_boxes_not_found = predict_and_match_to_gt(best_model, test_generator_real, test_frame_real, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect ground truth labels for faces that were not detected by the face detector\n",
    "gt_for_not_found = [x for x,y in zip(matched_gt, matched_scores) if y is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{matched_scores.count(None)} ground truth face annotations were not detected by the face detector\")\n",
    "print(f\"{gt_for_not_found.count(1)} of them were masked, {gt_for_not_found.count(0)} of them were not masked\")\n",
    "\n",
    "print(f\"{len(scores_for_detector_boxes_not_found)} boxes that were found by the face detector were not in the ground truth\")\n",
    "scores_for_detector_boxes_not_found_b = [x > 0.5 for x in scores_for_detector_boxes_not_found]\n",
    "print(f\"{scores_for_detector_boxes_not_found_b.count(1)} of these were predicted masked, {scores_for_detector_boxes_not_found_b.count(0)} of these were predicted unmasked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect ground truth and predictions for the detected bounding boxes\n",
    "matched_gt_not_none = [x for x,y in zip(matched_gt, matched_scores) if y is not None]\n",
    "matched_scores_not_none = [x for x in matched_scores if x is not None]\n",
    "matched_pred_not_none = [x > 0.5 for x in matched_scores_not_none]\n",
    "\n",
    "# collect ground truth and predictions for all ground truth bounding boxes, including 1 as a prediction if the ground-truth face was not detected\n",
    "matched_scores_none_replaced = [1 if x is None else x for x in matched_scores]\n",
    "matched_pred_none_replaced = [x > 0.5 for x in matched_scores_none_replaced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of the mask/no_mask classifier on the ground-truth faces that have been detected by the detector: {metrics.accuracy_score(matched_gt_not_none,matched_pred_not_none):.2f}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(matched_gt_not_none, matched_pred_not_none)\n",
    "plot_confusion_matrix(cm, ['not masked', 'masked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thr = metrics.roc_curve(matched_gt, matched_scores_none_replaced)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', size=15)\n",
    "plt.ylabel('True Positive Rate', size=15)\n",
    "plt.title(f'ROC for complete pipeline\\nAUC = {auc:.3f}', size=15)\n",
    "plt.savefig(performance_plots_dir / 'roc_complete.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting some examples. Black squares indicate ground truth face bounding boxes.  \n",
    "Green squares indicate bounding boxes produced by the detector and subsequently classified as masked.  \n",
    "Red squares indicate bounding boxes produced by the detector and subsequently classified as non-masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_plot = 16\n",
    "all_correct_pred = list(plot_dir_correct.glob('*.jpg'))\n",
    "to_plot = random.sample(all_correct_pred,n_to_plot)\n",
    "\n",
    "nrow = np.ceil(np.sqrt(n_to_plot))\n",
    "ncol = nrow\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, x in enumerate(to_plot):\n",
    "    plt.subplot(nrow, ncol, idx+1)\n",
    "    img = cv2.imread(str(x))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample of images with all detected faces classified correctly', size=20, y=0.9)\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_plot = 16\n",
    "all_mistake_pred = list(plot_dir_mistake.glob('*.jpg'))\n",
    "to_plot = random.sample(all_mistake_pred,n_to_plot)\n",
    "\n",
    "nrow = np.ceil(np.sqrt(n_to_plot))\n",
    "ncol = nrow\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, x in enumerate(to_plot):\n",
    "    plt.subplot(nrow, ncol, idx+1)\n",
    "    img = cv2.imread(str(x))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample of images with at least one mistake', size=20, y=0.9)\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
